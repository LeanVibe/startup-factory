#!/usr/bin/env python3
"""
Test-Driven Development for MVP Evolution System

FIRST PRINCIPLES:
1. Product-market fit requires systematic MVP iteration based on real customer data
2. Founders need specific, actionable improvement recommendations, not generic advice
3. MVP evolution must align with business model and customer validation insights

TDD APPROACH:
1. Write failing tests for MVP evolution and improvement suggestion
2. Implement minimal MVP evolution system
3. Validate with realistic business scenarios

FUNDAMENTAL TRUTH: 
Static MVPs don't achieve product-market fit. Systematic iteration based on customer data does.
"""

import asyncio
import pytest
from typing import Dict, List, Any, Optional
from dataclasses import dataclass, field
from datetime import datetime, timedelta
from enum import Enum

# Import from our existing systems and the actual implementation
from customer_feedback_system import CustomerFeedbackSystem, FeedbackResponse, FeedbackType
from customer_validation_dashboard import CustomerValidationDashboard, ValidationDashboard
from customer_acquisition_system import BusinessContext
from mvp_evolution_system import (
    MVPEvolutionSystem, MVPImprovement, MVPEvolutionPlan,
    ImprovementType, ImprovementPriority
)

class TestMVPEvolutionSystem:
    """TDD tests for MVP evolution system"""
    
    @pytest.fixture
    def healthcare_business_context(self):
        return {
            "business_model": "b2b_saas",
            "industry": "healthcare",
            "target_audience": "medical practices with 10-50 doctors",
            "value_proposition": "automated HIPAA compliance management",
            "key_features": ["compliance dashboard", "audit trails", "patient reporting"],
            "current_stage": "mvp_launched",
            "customers_acquired": 3,
            "monthly_revenue": 600
        }
    
    @pytest.fixture
    def mixed_feedback_data(self):
        """Realistic feedback data with pain points and requests"""
        return [
            FeedbackResponse(
                response_id="feedback_1",
                widget_id="widget_1",
                user_id="user_1", 
                session_id="session_1",
                feedback_type=FeedbackType.RATING,
                rating=3,
                text="Compliance dashboard is useful but loading takes 30+ seconds, very frustrating",
                sentiment="neutral",
                feature_mentioned="dashboard",
                submitted_at=datetime.now() - timedelta(days=2)
            ),
            FeedbackResponse(
                response_id="feedback_2",
                widget_id="widget_2",
                user_id="user_2",
                session_id="session_2",
                feedback_type=FeedbackType.OPEN_TEXT,
                rating=None,
                text="Need integration with our EMR system Epic, manual data entry is killing us",
                sentiment="negative",
                feature_mentioned="integration",
                submitted_at=datetime.now() - timedelta(days=1)
            ),
            FeedbackResponse(
                response_id="feedback_3",
                widget_id="widget_3", 
                user_id="user_3",
                session_id="session_3",
                feedback_type=FeedbackType.RATING,
                rating=4,
                text="Love the audit reports, but need automated email scheduling for monthly reports",
                sentiment="positive",
                feature_mentioned="reporting",
                submitted_at=datetime.now() - timedelta(days=3)
            ),
            FeedbackResponse(
                response_id="feedback_4",
                widget_id="widget_4",
                user_id="user_4",
                session_id="session_4", 
                feedback_type=FeedbackType.OPEN_TEXT,
                rating=None,
                text="Onboarding was confusing, took 3 hours to set up first patient record",
                sentiment="negative",
                feature_mentioned="onboarding",
                submitted_at=datetime.now() - timedelta(days=4)
            ),
            FeedbackResponse(
                response_id="feedback_5",
                widget_id="widget_5",
                user_id="user_5",
                session_id="session_5",
                feedback_type=FeedbackType.NPS,
                rating=7,
                text="Good start but pricing feels high for what we get, need more value",
                sentiment="neutral",
                submitted_at=datetime.now() - timedelta(days=1)
            )
        ]
    
    @pytest.fixture
    def sample_validation_dashboard(self, healthcare_business_context):
        """Sample validation dashboard with metrics"""
        dashboard_system = CustomerValidationDashboard()
        return dashboard_system.generate_dashboard(healthcare_business_context)
    
    def test_mvp_performance_analysis_from_data(self, healthcare_business_context, 
                                               sample_validation_dashboard, mixed_feedback_data):
        """
        TEST: System should analyze current MVP performance from validation data
        
        EXPECTED BEHAVIOR:
        - Identify performance bottlenecks from customer feedback
        - Analyze customer satisfaction metrics
        - Extract usage patterns and pain points
        - Assess product-market fit indicators
        """
        evolution_system = MVPEvolutionSystem()
        
        mvp_analysis = evolution_system.analyze_mvp_performance(
            healthcare_business_context,
            sample_validation_dashboard, 
            mixed_feedback_data
        )
        
        # Test analysis completeness
        assert isinstance(mvp_analysis, dict)
        assert "performance_issues" in mvp_analysis
        assert "user_satisfaction" in mvp_analysis
        assert "feature_feedback" in mvp_analysis
        assert "business_metrics" in mvp_analysis
        
        # Test specific insights from feedback data
        performance_issues = mvp_analysis["performance_issues"]
        assert len(performance_issues) > 0
        assert any("loading" in issue.lower() or "speed" in issue.lower() for issue in performance_issues)
        
        # Test feature analysis
        feature_feedback = mvp_analysis["feature_feedback"]
        assert "dashboard" in feature_feedback  # From feedback mentioning dashboard
        assert "reporting" in feature_feedback  # From feedback mentioning reporting
    
    def test_improvement_recommendations_from_feedback_analysis(self, healthcare_business_context, mixed_feedback_data):
        """
        TEST: System should generate specific improvement recommendations
        
        EXPECTED BEHAVIOR:
        - Performance improvements for reported speed issues
        - New features based on customer requests (EMR integration)
        - UX improvements based on user pain points (onboarding)
        - Business model optimizations based on pricing feedback
        """
        evolution_system = MVPEvolutionSystem()
        
        # Mock MVP analysis (in real system this comes from analyze_mvp_performance)
        mvp_analysis = {
            "performance_issues": ["Dashboard loading time 30+ seconds"],
            "feature_requests": ["Epic EMR integration", "Automated email scheduling"],
            "ux_pain_points": ["Confusing onboarding process"],
            "business_concerns": ["Pricing feels high for value provided"]
        }
        
        improvements = evolution_system.generate_improvement_recommendations(
            mvp_analysis, healthcare_business_context, mixed_feedback_data
        )
        
        # Test improvement generation
        assert isinstance(improvements, list)
        assert len(improvements) >= 3  # Should generate multiple improvements
        
        # Test improvement types
        improvement_types = [imp.improvement_type for imp in improvements]
        assert ImprovementType.PERFORMANCE_FIX in improvement_types  # For loading issues
        assert ImprovementType.NEW_FEATURE in improvement_types  # For EMR integration
        assert ImprovementType.UX_IMPROVEMENT in improvement_types  # For onboarding
        
        # Test improvement quality
        for improvement in improvements:
            assert isinstance(improvement, MVPImprovement)
            assert len(improvement.title) > 10  # Substantial titles
            assert len(improvement.description) > 20  # Detailed descriptions
            assert len(improvement.business_impact) > 15  # Clear business impact
            assert improvement.confidence_score > 0 and improvement.confidence_score <= 1
    
    def test_improvement_prioritization_by_business_impact(self, healthcare_business_context):
        """
        TEST: System should prioritize improvements by business impact and effort
        
        EXPECTED BEHAVIOR:
        - Critical issues (blocking customers) get CRITICAL priority
        - High ROI improvements get HIGH priority
        - Nice-to-haves get LOW priority
        - B2B businesses prioritize customer retention over acquisition features
        """
        evolution_system = MVPEvolutionSystem()
        
        # Mock improvements with different impact levels
        mock_improvements = [
            MVPImprovement(
                improvement_id="perf_1",
                improvement_type=ImprovementType.PERFORMANCE_FIX,
                priority=ImprovementPriority.MEDIUM,  # Will be re-prioritized
                title="Fix dashboard loading speed",
                description="Optimize database queries causing 30s load times",
                business_impact="Customer satisfaction and retention",
                effort_estimate="2-3 days",
                success_metrics=["Page load time < 3 seconds"],
                implementation_guidance="Optimize SQL queries and add caching",
                based_on_feedback=["Dashboard takes 30+ seconds to load"],
                confidence_score=0.95
            ),
            MVPImprovement(
                improvement_id="feature_1", 
                improvement_type=ImprovementType.NEW_FEATURE,
                priority=ImprovementPriority.MEDIUM,
                title="Epic EMR integration",
                description="Direct integration with Epic EMR system",
                business_impact="Major competitive advantage, customer acquisition",
                effort_estimate="3-4 weeks",
                success_metrics=["20% faster data entry", "Customer acquisition rate +50%"],
                implementation_guidance="Use Epic FHIR APIs for data sync",
                based_on_feedback=["Need Epic integration"],
                confidence_score=0.85
            )
        ]
        
        prioritized_improvements = evolution_system.prioritize_improvements(
            mock_improvements, healthcare_business_context
        )
        
        # Test prioritization logic
        assert isinstance(prioritized_improvements, list)
        assert len(prioritized_improvements) == len(mock_improvements)
        
        # Performance issues should be high priority (affect all customers)
        perf_improvement = next(imp for imp in prioritized_improvements if imp.improvement_id == "perf_1")
        assert perf_improvement.priority in [ImprovementPriority.CRITICAL, ImprovementPriority.HIGH]
        
        # High-impact new features should be prioritized appropriately
        feature_improvement = next(imp for imp in prioritized_improvements if imp.improvement_id == "feature_1")
        assert feature_improvement.priority in [ImprovementPriority.HIGH, ImprovementPriority.MEDIUM]
    
    def test_complete_evolution_plan_creation(self, healthcare_business_context, 
                                            sample_validation_dashboard, mixed_feedback_data):
        """
        TEST: System should create complete MVP evolution plan
        
        EXPECTED BEHAVIOR:
        - Comprehensive plan with prioritized improvements
        - Next sprint recommendations (3-5 top priority items)
        - Clear iteration focus aligned with business goals
        - Expected business impact from implementing the plan
        """
        evolution_system = MVPEvolutionSystem()
        
        evolution_plan = evolution_system.create_evolution_plan(
            healthcare_business_context,
            sample_validation_dashboard,
            mixed_feedback_data
        )
        
        # Test plan creation
        assert isinstance(evolution_plan, MVPEvolutionPlan)
        assert evolution_plan.plan_id is not None and len(evolution_plan.plan_id) > 0
        assert evolution_plan.business_context == healthcare_business_context
        
        # Test plan completeness
        assert len(evolution_plan.improvements) >= 3  # Multiple improvements
        assert len(evolution_plan.next_sprint_recommendations) >= 3  # Sprint-sized recommendations
        assert len(evolution_plan.next_sprint_recommendations) <= 5  # Not overwhelming
        assert len(evolution_plan.expected_impact) > 20  # Substantial impact description
        assert len(evolution_plan.iteration_focus) > 15  # Clear focus description
        
        # Test business context alignment
        assert "healthcare" in evolution_plan.iteration_focus.lower() or "hipaa" in evolution_plan.iteration_focus.lower()
        assert "b2b" in evolution_plan.expected_impact.lower() or "practice" in evolution_plan.expected_impact.lower()
    
    def test_next_sprint_planning_with_realistic_scope(self, healthcare_business_context,
                                                      sample_validation_dashboard, mixed_feedback_data):
        """
        TEST: System should generate realistic next sprint recommendations
        
        EXPECTED BEHAVIOR:
        - 3-5 items that can be completed in 2-week sprint
        - Mix of quick wins and important features
        - Clear, actionable items founders can implement
        - Prioritized by business impact and implementability
        """
        evolution_system = MVPEvolutionSystem()
        
        evolution_plan = evolution_system.create_evolution_plan(
            healthcare_business_context,
            sample_validation_dashboard,
            mixed_feedback_data
        )
        
        sprint_plan = evolution_system.get_next_sprint_plan(evolution_plan)
        
        # Test sprint plan quality
        assert isinstance(sprint_plan, list)
        assert len(sprint_plan) >= 3 and len(sprint_plan) <= 5  # Realistic sprint scope
        
        # Test actionability
        for item in sprint_plan:
            assert len(item) > 15  # Substantial recommendations
            assert any(action_word in item.lower() for action_word in [
                "fix", "improve", "optimize", "implement", "add", "update", "create"
            ])  # Contains actionable verbs
    
    def test_business_model_specific_recommendations(self, healthcare_business_context, mixed_feedback_data):
        """
        TEST: Recommendations should be tailored to business model and industry
        
        EXPECTED BEHAVIOR:
        - B2B SaaS recommendations focus on customer retention and expansion
        - Healthcare-specific compliance and integration recommendations
        - Industry-appropriate success metrics and implementation guidance
        - Business model-aligned prioritization (B2B focuses on customer success over user acquisition)
        """
        evolution_system = MVPEvolutionSystem()
        
        # Mock analysis with B2B healthcare context
        mvp_analysis = {
            "business_health": {"customer_churn_risk": "medium", "expansion_opportunity": "high"},
            "feature_requests": ["Epic integration", "Automated reporting"],
            "compliance_concerns": ["HIPAA audit readiness"]
        }
        
        improvements = evolution_system.generate_improvement_recommendations(
            mvp_analysis, healthcare_business_context, mixed_feedback_data
        )
        
        # Test B2B SaaS focus
        business_impacts = [imp.business_impact.lower() for imp in improvements]
        b2b_terms = ["retention", "churn", "expansion", "customer success", "account growth"]
        assert any(any(term in impact for term in b2b_terms) for impact in business_impacts)
        
        # Test healthcare industry focus
        healthcare_terms = ["hipaa", "compliance", "emr", "medical", "practice"]
        improvement_text = " ".join([imp.title + " " + imp.description for imp in improvements]).lower()
        assert any(term in improvement_text for term in healthcare_terms)
    
    def test_confidence_scoring_based_on_data_quality(self, healthcare_business_context):
        """
        TEST: System should provide confidence scores based on data quality
        
        EXPECTED BEHAVIOR:
        - High confidence for improvements backed by multiple customer complaints
        - Lower confidence for improvements based on single feedback point
        - Confidence scoring considers feedback sentiment and specificity
        - Business impact confidence based on data richness
        """
        evolution_system = MVPEvolutionSystem()
        
        # High-confidence scenario: Multiple customers complaining about same issue
        high_confidence_feedback = [
            FeedbackResponse(
                response_id="f1", widget_id="w1", user_id="u1", session_id="s1",
                feedback_type=FeedbackType.RATING, rating=2,
                text="Dashboard is way too slow, takes forever to load",
                sentiment="negative", feature_mentioned="dashboard"
            ),
            FeedbackResponse(
                response_id="f2", widget_id="w2", user_id="u2", session_id="s2",
                feedback_type=FeedbackType.RATING, rating=3,
                text="Love the features but loading time is frustrating",
                sentiment="neutral", feature_mentioned="dashboard"
            ),
            FeedbackResponse(
                response_id="f3", widget_id="w3", user_id="u3", session_id="s3",
                feedback_type=FeedbackType.OPEN_TEXT,
                text="Please fix the slow dashboard performance",
                sentiment="negative", feature_mentioned="dashboard"
            )
        ]
        
        mvp_analysis = {"performance_issues": ["Dashboard loading speed"]}
        improvements = evolution_system.generate_improvement_recommendations(
            mvp_analysis, healthcare_business_context, high_confidence_feedback
        )
        
        # Find dashboard performance improvement
        dashboard_improvement = next((imp for imp in improvements 
                                    if "dashboard" in imp.title.lower() and "performance" in imp.title.lower()), None)
        
        assert dashboard_improvement is not None
        assert dashboard_improvement.confidence_score >= 0.8  # High confidence due to multiple complaints

async def run_comprehensive_mvp_evolution_tests():
    """Run the comprehensive TDD test suite for MVP evolution"""
    
    print("üîÑ MVP EVOLUTION SYSTEM - TDD VALIDATION")
    print("=" * 70)
    print("Testing systematic MVP iteration based on customer feedback")
    print()
    
    # Create test scenario
    business_context = {
        "business_model": "b2b_saas",
        "industry": "healthcare",
        "target_audience": "medical practices",
        "key_features": ["compliance dashboard", "audit trails"],
        "customers_acquired": 3
    }
    
    # Initialize system (this will fail initially - that's the point of TDD)
    evolution_system = MVPEvolutionSystem()
    
    test_scenarios = [
        ("MVP Performance Analysis", "performance_analysis"),
        ("Improvement Recommendations", "recommendations"), 
        ("Improvement Prioritization", "prioritization"),
        ("Complete Evolution Plan", "evolution_plan"),
        ("Next Sprint Planning", "sprint_planning"),
        ("Business Model Alignment", "business_alignment"),
        ("Confidence Scoring", "confidence")
    ]
    
    print("üî• RUNNING TDD TESTS (These should FAIL initially)")
    print("-" * 50)
    
    failed_tests = 0
    
    for scenario_name, test_type in test_scenarios:
        print(f"\nüìã Testing: {scenario_name}")
        
        try:
            if scenario_name == "MVP Performance Analysis":
                # Mock validation dashboard and feedback
                dashboard_system = CustomerValidationDashboard()
                dashboard = dashboard_system.generate_dashboard(business_context)
                feedback_data = []  # Empty for simplicity
                
                analysis = evolution_system.analyze_mvp_performance(
                    business_context, dashboard, feedback_data
                )
                print("   ‚úÖ MVP performance analysis completed")
                
            elif scenario_name == "Improvement Recommendations":
                mvp_analysis = {"performance_issues": ["slow loading"]}
                feedback_data = []
                
                improvements = evolution_system.generate_improvement_recommendations(
                    mvp_analysis, business_context, feedback_data
                )
                print("   ‚úÖ Improvement recommendations generated")
                
            elif scenario_name == "Improvement Prioritization":
                mock_improvements = []  # Would contain MVPImprovement objects
                prioritized = evolution_system.prioritize_improvements(
                    mock_improvements, business_context
                )
                print("   ‚úÖ Improvement prioritization completed")
                
            elif scenario_name == "Complete Evolution Plan":
                dashboard_system = CustomerValidationDashboard()
                dashboard = dashboard_system.generate_dashboard(business_context)
                feedback_data = []
                
                plan = evolution_system.create_evolution_plan(
                    business_context, dashboard, feedback_data
                )
                print("   ‚úÖ Evolution plan created")
                
            elif scenario_name == "Next Sprint Planning":
                # Mock evolution plan with proper attributes
                mock_plan = type('MockPlan', (), {
                    'improvements': [],
                    'business_context': business_context,
                    'next_sprint_recommendations': ["Test sprint item 1", "Test sprint item 2"]
                })()
                
                sprint_plan = evolution_system.get_next_sprint_plan(mock_plan)
                print("   ‚úÖ Sprint planning completed")
                
            else:
                # Generic test
                print("   ‚úÖ Test scenario completed")
                
        except NotImplementedError as e:
            print(f"   ‚ùå EXPECTED FAILURE: {e}")
            failed_tests += 1
        except Exception as e:
            print(f"   ‚ùå UNEXPECTED ERROR: {e}")
            failed_tests += 1
    
    print(f"\nüìä TDD RESULTS:")
    print(f"Failed Tests: {failed_tests} (Expected)")
    print(f"Total Scenarios: {len(test_scenarios)}")
    
    if failed_tests == len(test_scenarios):
        print("\n‚úÖ TDD SETUP CORRECT!")
        print("Tests are failing as expected - now we implement the features.")
        print("\nNext Step: Implement MVPEvolutionSystem methods to make tests pass")
    else:
        print("\n‚ùå TDD SETUP ISSUE!")
        print("Some tests passed unexpectedly - check implementation")
    
    return {
        "setup_correct": failed_tests == len(test_scenarios),
        "failed_tests": failed_tests,
        "total_tests": len(test_scenarios)
    }

if __name__ == "__main__":
    print("üîÑ MVP EVOLUTION SYSTEM - TDD SETUP")
    print("Setting up failing tests to drive implementation")
    print()
    
    # Run TDD setup - tests should fail initially
    results = asyncio.run(run_comprehensive_mvp_evolution_tests())
    
    print("\n" + "=" * 70)
    print("üìã TDD METHODOLOGY APPLIED")
    print()
    print("1. ‚ùå Tests written that define expected behavior (FAILING)")
    print("2. ‚è≥ Next: Implement minimal code to make tests pass") 
    print("3. ‚è≥ Then: Refactor while keeping tests green")
    print()
    print("üîÑ MVP EVOLUTION SYSTEM READY FOR IMPLEMENTATION")